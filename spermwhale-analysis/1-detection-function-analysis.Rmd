---
title: Detection function analysis of sperm whale data
author: David L Miller

---

# Preamble

First need to load the requisite R libraries

```{r load-libraries, messages=FALSE}
library(rgdal)
library(ggplot2)
library(Distance)
library(knitr)
```

# Load the data

The observations are located in a "geodatabase" we created in Arc. We want to pull out the "Sightings" table and make it into a `data.frame` (so that it's easier for R to manipulate). We can check it has the correct format using `head`:

```{r load-data}
distdata <- readOGR("Analysis.gdb","Sightings")
distdata <- as.data.frame(distdata)
head(distdata)
```

The `Distance` package expects certain column names to be used. This is much easier to do in R than ArcGIS, so we do it here.
```{r rename-cols}
distdata$distance <- distdata$Distance
distdata$object <- distdata$SightingID
distdata$Sample.Label <- distdata$SegmentID
distdata$size <- distdata$GroupSize
```


# Exploratory analysis

Before setting off fitting detection functions, let's look at the relationship of various variables in the data.

## Distances

Obviously, the most important covariate in a distance sampling analysis is distance itself. We can plot a histogram of the distances to check that (1) we imported the data correctly and (2) it conforms to the usual line transect shape.

```{r eda-dist}
hist(distdata$distance, xlab="Distance (m)")
```


## Size and distance

We might expect that there will be a relationship between the distance at whcih we see animals and the size of the groups observed (larger groups are easier to see at larger distances), so let's plot that and use `loess` and `lm` to fit models to the relationship to help us visualise the relationship.

```{r eda-covars}
par(mfrow=c(1,2))
# plot of size versus distance, linear model and LOESS smoother overlay
plot(distdata[c("size","distance")], xlab="Group size", ylab="Distance (m)",pch=19,col=rgb(0,0,0,0.4), cex=0.6)
# increase span from default 0.75 for slightly smoother curve
lo <- loess(distance ~ size, distdata, span=0.8)
lmm <- lm(distance ~ size, distdata)
preddat <- data.frame(size=seq(0,8,1))
lines(x=preddat$size, y=predict(lmm, preddat),lty=2)
lines(x=preddat$size, y=predict(lo, preddat))

plot(distdata[c("SeaState","distance")], xlab="Beaufort sea state", ylab="Distance (m)",pch=19,col=rgb(0,0,0,0.4), cex=0.6)
# increase span from default 0.75 for slightly smoother curve
lo <- loess(distance ~ SeaState, distdata, span=0.8)
lmm <- lm(distance ~ SeaState, distdata)
preddat <- data.frame(SeaState=seq(0,8,1))
lines(x=preddat$SeaState, y=predict(lmm, preddat),lty=2)
lines(x=preddat$SeaState, y=predict(lo, preddat))
```

## Distance and sea state

We might also expect that increaing sea state would result in a drop in observations. We can plot histograms of distance for each sea state level (making the sea state take only values 0,1,2,4,5 for this).

```{r eda-dist-facet-seastate}
distdata$SeaStateCut <- cut(distdata$SeaState,seq(0,5,by=1), include.lowest=TRUE)
p <- ggplot(distdata)
p <- p + geom_histogram(aes(distance))
p <- p + facet_wrap(~SeaStateCut)
print(p)
```

## Survey effect

Given we are including data from two different surveys we can also investigate the relationship between survey and distances observed.

```{r eda-dist-facet-survey}
p <- ggplot(distdata)
p <- p + geom_histogram(aes(distance))
p <- p + facet_wrap(~Survey)
print(p)
```




# Fitting detection functions



Here we fit all possible detection function mdoels with half-normal and hazard-rate functions using each combination of covariates. This is somewhat brute force.

```{r df-models}
models <- list()

# half-normal models
models$hn <- ds(distdata, truncation=6000, adjustment=NULL)
models$hn.ss <- ds(distdata, truncation=6000, adjustment=NULL, formula=~SeaState)
models$hn.size <- ds(distdata, truncation=6000, adjustment=NULL, formula=~size)
models$hn.ss.size <- ds(distdata, truncation=6000, adjustment=NULL, formula=~size+SeaState)
models$hn.survey <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey))
models$hn.survey.ss <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey)+SeaState)
models$hn.survey.size <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey)+size)
models$hn.survey.size.ss <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey)+size+SeaState)

# hazard-rate models
models$hr <- ds(distdata, truncation=6000, adjustment=NULL, key="hr")
models$hr.ss <- ds(distdata, truncation=6000, adjustment=NULL, formula=~SeaState, key="hr")
models$hr.size <- ds(distdata, truncation=6000, adjustment=NULL, formula=~size, key="hr")
models$hr.ss.size <- ds(distdata, truncation=6000, adjustment=NULL, formula=~size+SeaState, key="hr")
models$hr.survey <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey), key="hr")
models$hr.survey.ss <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey)+SeaState, key="hr")
models$hr.survey.size <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey)+size, key="hr")
models$hr.survey.size.ss <- ds(distdata, truncation=6000, adjustment=NULL, formula=~as.factor(Survey)+size+SeaState, key="hr")
```

This code will make a results table with relevant statistics for model selection in it.

```{r support-make-table}
make_table <- function(models){

  extract_model_data <- function(model){
    c(summary(model)$ds$key,
      model$ddf$ds$aux$ddfobj$scale$formula,
      model$ddf$criterion,
      ddf.gof(model$ddf, qq=FALSE)$dsgof$CvM$p,
      summary(model)$ds$average.p,
      summary(model)$ds$average.p.se
    )
  }

  res <- as.data.frame(t(as.data.frame(lapply(models, extract_model_data))),
                        stringsAsFactors=FALSE)

  res[,3] <- as.numeric(res[,3])
  res[,4] <- as.numeric(res[,4])
  res[,5] <- as.numeric(res[,5])
  res[,6] <- as.numeric(res[,6])

  colnames(res) <- c("Key function", "Formula", "AIC", "Cramer-von Mises $p$-value",
                     "$\\hat{P_a}$", "se($\\hat{P_a}$)")

  res[["$\\Delta$AIC"]] <- res$AIC - min(res$AIC, na.rm=TRUE)
  res <- res[order(res$AIC),]

  res
}
```

```{r df-results, results="asis"}
kable(make_table(models), digits=3)
```

Plot the best 4 models:

```{r plot-top-models, fig.width=6, fig.height=6}
par(mfrow=c(2,2))
plot(models$hr.ss, main="hr.ss")
plot(models$hr, main="hr")
plot(models$hr.survey.ss, main="hr.survey.ss")
plot(models$hn.ss, main="hn.ss")
```

We can produce the corresponding quantile-quantile plots and goodness of fit test results

```{r qqplot-top-models, fig.width=6, fig.height=6}
par(mfrow=c(2,2))
ddf.gof(models$hr.ss$ddf, main="hr.ss")
ddf.gof(models$hr$ddf, main="hr")
ddf.gof(models$hr.survey.ss$ddf, main="hr.survey.ss")
ddf.gof(models$hn.ss$ddf, main="hn.ss")
```

## Save the "best" models

Let's save `hr` and `hr.ss` for now...

```{r save-models}
df_hr_ss <- models$hr.ss
df_hr <- models$hr
save(df_hr_ss, df_hr, distdata, file="df-models.RData")
```

## Estimating abundance

Just for fun, let's estimate abundance from these models using a Horvtiz-Thompson-type estimator.

We know the Horvitz-Thompson estimator has the following form:
$$
\hat{N} = \frac{A}{a} \sum_{i=1}^n \frac{s_i}{p_i}
$$
we can calculate each part of this equation in R:

First we need to load the effort data from the geodatabase, this is in the "Segments" table. We can then use this to build the "`sample.table`"
```{r get-sample-table}
segs <- readOGR("Analysis.gdb", "Segments")
segs <- as.data.frame(segs)
segs$Shape_Length <- NULL
segs$BeginTime <- NULL
segs$CenterTime <- NULL
segs$EndTime <- NULL
segs$FIRST_Survey <- NULL

library(plyr)
segs2 <- segs
segs2$SegmentID <- NULL
sample.table <- ddply(segs2, .(EffortDayID), summarize, Transect.Length=sum(Length))
names(sample.table) <- c("Sample.Label", "Effort")
sample.table$Region.Label <- "StudyArea"
```

The total area of the EEZ is $A=5.285e+11$, we can use that to construct the region table, with one region in it:
```{r get-region-table}
region.table <- data.frame(Region.Label = "StudyArea",
                           Area         = 5.285e+11)
```

Finally we can construct the observation table to link the samples to the detection function:
```{r get-obs-table}
segs$Sample.Label <- segs$SegmentID
obs.table <- join(distdata, segs, by="Sample.Label")
distdata$Sample.Label <- NULL
obs.table$Sample.Label <- obs.table$EffortDayID
obs.table <- obs.table[,c("object","Sample.Label")]
obs.table$Region.Label <- "StudyArea"
```

```{r ht}
Nhat_hr <- dht(df_hr$ddf, region.table, sample.table, obs.table)
```

Now replacing `p` with the values from the `df_hr_ss` model we can make an estimate for that model too:
```{r ht-cov}
Nhat_hr_ss <- dht(df_hr_ss$ddf, region.table, sample.table, obs.table)
```


### Accounting for perception bias

From Palka (2006), we think that observations on the track line were such that $g(0)=0.46$, we can apply that correction to our abundance estimate (in a very primitive way):

```{r ht-perception}
Nhat_hr$individuals$N$Estimate/0.46
```

```{r ht-cov-perception}
Nhat_hr_ss$individuals$N$Estimate/0.46
```



## References


Palka, D. (2006). Summer Abundance Estimates of Cetaceans in US North Atlantic Navy Operating Areas. Northeast Fisheries Science Center Reference Document 06-03.