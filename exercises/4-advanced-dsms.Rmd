---
title: 'Practical 4: Density surface models with multiple smooths, model selection'
author: David L Miller

---

# Aims

By the end of this practical, you should feel comfortable:

- Fittin DSMs with multiple smooth terms in them
- Selecting smooth terms by $p$-values
- Using shrinkage smoothers
- Selecting between models using deviance, REML score
- Investigating concurvity in DSMs with multiple smooths
- Investigating sensitivity sensitivity and path dependence

# Load data and packages

```{r load-packages}
library(Distance)
library(dsm)
library(ggplot2)
library(knitr)
```

Loading the data processed from GIS and the fitted detection function objects from the previous exercises:
```{r load-data}
load("sperm-data.RData")
load("df-models.RData")
```


# Our new friend `+`

We can build a really big model using `+` to include all the terms that we want in the model. We can check what's available to us by using `head()` to look at the segment table:
```{r seg-table}
head(segs)
```

We can then fit a model with the available covariates in it, each as an `s()` term.
```{r nb-xy}
dsm_nb_xy_ms <- dsm(count~s(x,y, bs="ts") +
                       s(Depth, bs="ts") +
                       s(DistToCAS, bs="ts") +
                       s(SST, bs="ts") +
                       s(EKE, bs="ts") +
                       s(NPP, bs="ts"),
                 df_hn, segs, obs,
                 family=nb(), method="REML")
summary(dsm_nb_xy_ms)
```

Notes:

1. We're using `bs="ts"` to use the shrinkage thin plate regression spline. More technical detail on these smooths can be found on their manual page `?smooth.construct.ts.smooth.spec`.
2. We've not specified basis complexity (`k`) at the moment. Note that if you want to specify the same complexity for multiple terms, it's often easier to make a variable that can then be given as `k` (for example, setting `k1<-15` and then setting `k=k1` in the required `s()` terms).

## Plot

Let's plot the smooths from this model:

```{r plot-nb-xy}
plot(dsm_nb_xy_ms, pages=1, scale=0)
```

Notes:

1. Setting `shade=TRUE` gives prettier confidence bands.
2. As with `vis.gam()` the response is on the link scale.
3. `scale=0` puts each plot on a different $y$-axis scale, making it easier to see the effects. Setting `scale=-1` will put the plots on a common $y$-axis scale


We can also plot the bivariate smooth of `x` and `y` as we did before, using `vis.gam()`:
```{r nb-xy-visgam, fig.width=5, fig.height=5}
vis.gam(dsm_nb_xy_ms, view=c("x","y"), plot.type="contour", too.far=0.1, main="s(x,y) (link scale)", asp=1)
```

Compare this plot to the one that was generated in the previous practical when only `x` and `y` were included in the model.

## Check

As before, we can use `gam.check()` and `rqgam.check()` to look at the residual check plots for this model. Do this in the below gaps and comment on the resulting plots and diagnostics.

```{r check-nb-xy}

```

```{r rqcheck-nb-xy}

```

You might decide from the diagnostics that you need to increase `k` for some of the terms in the model. Do this and re-run the above code to ensure that the smooths are flexible enough. The `?choose.k` manual page can offer some guidance. Generally if teh EDF is close to the value of `k` you supplied, it's worth doubling `k` and refitting to see what happens. You can always switch back to the smaller `k` if there is little difference.

## Select terms

As was covered in the lectures, we can select terms by (approximate) $p$-values and by looking for terms that have EDFs significantly less than 1 (those which have been shrunk).

Decide on a significance level that you'll use to discard terms in the model. Remove the terms that are non-significant at this level and re-run the above checks, summaries and plots to see what happens. It's helpful to make notes to yourself as you go 

It's easiest to either comment out the terms that are to be removed (using `#`) or by copying the code chunk above and pasting it below.

Having removed a smooth and reviewed your model, you may decide you wish to remove another. Follow the process again, removing a term, looking at plots and diagnostics.



## Compare response distributions

Use the `gam.check()` to compare quantile-quantile plots between negative binomial and Tweedie distributions for the response.




# Estimated abundance as a response

Again, we've only looked at models with `count` as the response. Try using a detection function with covariates and the `abundance.est` response in the chunk below:

```{r abund-est}

```

# Concurvity

Checking concurvity of terms in the model can be accomplished using the `concurvity()` function.

```{r concurvity-checks}
concurvity(dsm_nb_xy_ms)
```

By default the function returns a matrix of a measure of concurvity between one of the terms and the rest of the model.

Compare the output of the models before and after removing terms.

Reading these matrices can be a little laborious and not very fun. Here is a little bit of code to visualise the concurvity *between terms* in a model by colour coding the matrix (and blanking out the redundant information).

```{r vis-concurvity-fn}
vis.concurvity <- function(b, type="estimate"){
  # arguments:
  #   b     -- a fitted gam
  #   type  -- concurvity measure to plot, see ?concurvity
  cc <- concurvity(b, full=FALSE)[[type]]

  diag(cc) <- NA
  cc[lower.tri(cc)]<-NA

  layout(matrix(1:2, ncol=2), widths=c(5,1))
  opar <- par(mar=c(5, 6, 5, 0) + 0.1)
  # main plot
  image(z=cc, x=1:ncol(cc), y=1:nrow(cc), ylab="", xlab="",
        axes=FALSE, asp=1, zlim=c(0,1))
  axis(1, at=1:ncol(cc), labels = colnames(cc), las=2)
  axis(2, at=1:nrow(cc), labels = rownames(cc), las=2)
  # legend
  opar <- par(mar=c(5, 0, 4, 3) + 0.1)
  image(t(matrix(rep(seq(0, 1, len=100), 2), ncol=2)),
        x=1:3, y=1:101, zlim=c(0,1), axes=FALSE, xlab="", ylab="")
  axis(4, at=seq(1,101,len=5), labels = round(seq(0,1,len=5),1), las=2)
  par(opar)
}
```

Again compare the results of plotting for models with different terms.


# Sensitivity

## Compare bivariate and additive spatial effects

If we replace the bivariate smooth of location (`s(x, y)`) with an additive terms (`s(x)+s(y)`), we may see a difference in the final model (different covariates selected).

```{r nb-x-y}
dsm_nb_x_y_ms <- dsm(count~s(x, bs="ts") +
                        s(y, bs="ts") +
                        s(Depth, bs="ts") +
                        s(DistToCAS, bs="ts") +
                        s(SST, bs="ts") +
                        s(EKE, bs="ts") +
                        s(NPP, bs="ts"),
                  df_hn, segs, obs,
                  family=nb(), method="REML")
summary(dsm_nb_x_y_ms)
```

Try performing model selection as before from this base model and compare the resulting models.

Compare the resulting smooths from like terms in the model. For example, if depth were selected in both models, compare EDFs and plots, e.g.:

```{r compare-depth}
par(mfrow=c(1,2))
plot(dsm_nb_xy_ms, select=2)
plot(dsm_nb_x_y_ms, select=3)
```

Note that there `select=` picks just one term to plot. These are in the order in which the terms occur in the `summary()` output (so you may well need to adjust the above code).


# Comparing models

As with the detection functions in the earlier exercises, here is a quick function to generate model results tables with appropriate summary statistics:

```{r summarize-models}
summarize_dsm <- function(model){

  summ <- summary(model)

  data.frame(response = model$family$family,
             terms    = paste(rownames(summ$s.table), collapse=", "),
             AIC      = AIC(model),
             REML     = model$gcv.ubre,
             "Deviance_explained" = paste0(round(summ$dev.expl*100,2),"%")
            )

}
```

We can, again, make a list of the models and give that to the above function

```{r apply-summary}
# add your models to this list!
model_list <- list(dsm_nb_x_y_ms, dsm_nb_xy_ms)
library(plyr)
summary_table <- ldply(model_list, summarize_dsm)
row.names(summary_table) <- c("dsm_nb_x_y_ms", "dsm_nb_xy_ms")
```

```{r print-table, results="asis"}
summary_table <- summary_table[order(summary_table$REML, decreasing=TRUE),]
kable(summary_table)
```

# Saving models

Now save the models that you'd like to use to predict with later. I recommend saving as many models as you can so you can compare their results in the next practical.

```{r save-models}
# add your models here
save(dsm_nb_xy_ms, dsm_nb_x_y_ms,
     file="dsms.RData")
```
